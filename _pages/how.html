---
layout: page
title: How does it work
permalink: /how-it-work/
---
<html>
    <link rel="stylesheet" href="/public/css/extra.css"> 
    <link rel="stylesheet" href="/public/css/timeline.css">
<body>
    <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

    <style>.tab { margin-left: 40px; }</style>
    <div id="hexagons" >
        <ul id="categories" class="clr">
        <li>
            <div class="flip-container" ontouchstart="this.classList.toggle('hover');">
            <div class="flipper">
                <div class="front" style="background-color: #1c87c9;">
                    <!-- <img src="/images/lego_base8.png"/> -->
                    <div class="flip-content title-xs">
                    <p>N.E.A.T. User Guide</p>
                </div>
                </div>
                <div class="back">
                    <!-- <img src="/images/Logo.png"/> -->
                    <div class="flip-content">
                        <p></p>
                        <p>
                        <a href="/archive/user_guide.pdf">Download user guide <i class="fa fa-arrow-circle-right"></i></a>
                        </p>
                    </div>
                </div>
            </div>
            </div>
        </li>
        <li>
            <div class="flip-container" ontouchstart="this.classList.toggle('hover');">
            <div class="flipper">
                <!-- <div class="front" style="background-color: #ff6347;"> -->
                    <div class="front">
                        <img src="/images/base_unlabeled.jpg"/>
                <div class="flip-content title-xs">
                    <p>Robot base</p>
                </div>
                </div>
                <div class="back">
                <div class="flip-content">
                    <p></p>
                    <p>
                    <a href="#base">read more <i class="fa fa-arrow-circle-right"></i></a>
                    </p>
                </div>
                </div>
            </div>
            </div>
        </li>
        <li>
            <div class="flip-container" ontouchstart="this.classList.toggle('hover');">
            <div class="flipper">
                <div class="front" >
                    <img src="/images/lift_retracted.jpg"/>
                    <div class="flip-content title-xs">
                        <p>Robot lift</p>
                    </div>
                </div>
                <div class="back">
                <div class="flip-content lg">
                    <p></p>
                    <p>
                    <a href="#lift">read more <i class="fa fa-arrow-circle-right" ></i></a>
                    </p>
                </div>
                </div>
            </div>
            </div>
        </li>
        <li>
            <div class="flip-container" ontouchstart="this.classList.toggle('hover');">
            <div class="flipper">
                <div class="front" style="background-color: #8ebf42;" >
                    <div class="flip-content title-xs">
                        <p >Raspberry pi & EV3</p>
                    </div>
                </div>
                <div class="back">
                <div class="flip-content md">
                    <p></p>
                    <p>
                    <a href="#pi_ev3">read more <i class="fa fa-arrow-circle-right"></i></a>
                    </p>
                </div>
                </div>
            </div>
            </div>
        </li>
        <li>
            <div class="flip-container" ontouchstart="this.classList.toggle('hover');">
            <div class="flipper">
                <div class="front" style="background-color: #ffcc00;">
                    <div class="flip-content title-xs">
                        <p>Mobile app</p>
                    </div>
                </div>
                <div class="back">
                <div class="flip-content lg">
                    <p></p>
                    <p>
                    <a href="#app">read more <i class="fa fa-arrow-circle-right"></i></a>
                    </p>
                </div>
                </div>
            </div>
            </div>
        </li>
        <li>
        </ul>
    </div>
    <div></div>

    <hr style="height:3px;border:none;color:#333;background-color:#333;">
    <a name="base"></a>
    <center><h1>Robot base</h1></center>
    <h3>Base Features</h3>
      <ul>
          <li>Provides a strong reinforced base that holds the rest of final product.</li>
          <li>Has the means for directional travel in up to 8 directions and can rotate on the spot.</li>
          <li>Securely houses the brains of the N.E.A.T. (EV3 and Raspberry Pi).</li>
          <li>Contains on-board object detection and avoidance.</li>
          <li>Provides power to the EV3, Raspberry Pi and Lift.</li>
      </ul>
    <h3>Robot base design timeline</h3>

    <ul class="timeline">
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 1</span>
                <span class="time-wrapper"><span class="time">January 15th</span></span>
            </div>
            <div class="desc">The first iteration of the robot was a simple Lego base which had 4 wheels and a very fragile frame that spanned roughly 20cm by 20cm. It was 
                created during the Robot Building Workshop.</div>
        </div>
    </li>
    <li>
        <div class="direction-l">
            <div class="flag-wrapper">
                <span class="flag">Iterations 2 and 3</span>
                <span class="time-wrapper"><span class="time">January 16-20th</span></span>
            </div>
            <div class="desc"><img src="/images/ivy_sketch.png"/> <img src="/images/barna_design.png"/>The next iterations consisted of a simple sketch of the robot and a more extensive graphical design. This iteration in the design project gave 
                us a projection of how we would like the final product to look and allowed us to explore a more realistic view of the final product.</div>
        </div>
    </li>
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 4</span>
                <span class="time-wrapper"><span class="time">January 21st</span></span>
            </div>
            <div class="desc">The base by this point consisted of a small, flat lego platform with the wheels attached, controlled by 2 lego motors and the EV3.</div>
        </div>
    </li>
    <li>
        <div class="direction-l">
            <div class="flag-wrapper">
                <span class="flag">Iteration 5</span>
                <span class="time-wrapper"><span class="time">January 27th</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base1.png"/> This iteration was similar to the previous iteration with the exception of the arrival of the Mechanum wheels which saw the base vastly improve by providing a testing platform for the development of the EV3 movement code.</div>
        </div>
    </li>
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 6</span>
                <span class="time-wrapper"><span class="time">January 30th</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base2.png"/> The base has increased in size significantly with considerable structural improvement. This base helped develop the EV3 code to control directional movements as well as rotation on the spot. A single ultrasonic sensor was added to aid the development and testing of the object detection and avoidance code.</div>
        </div>
    </li>
    <li>
        <div class="direction-l">
            <div class="flag-wrapper">
                <span class="flag">Iteration 7</span>
                <span class="time-wrapper"><span class="time"> Febuary 1st</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base3.png"/> Issues with the previous iteration of the robot base such as structural intregrity under weight resulted in this iteration being heavily reinforced with several layers of lego blocks. Furthermore, lego struts were placed along the inside to provide stability and support for the EV3 block to rest on during functionality. However, a major design flaw was discovered after completion, in that the wheels were not aligned and therefore rotational and diagonal movement was subpar.</div>
        </div>
    </li>
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 8</span>
                <span class="time-wrapper"><span class="time">Febuary 2nd</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base4.png"/> The base was taken apart and the length was increased to align the wheels correctly and fix the issue of the of the previous iteration. Upon weight testing, the base still showed significant signs of weakness, especially the corner sections, which were susceptible to breaking under reasonable weight.</div>
        </div>
    </li>
    <li>
        <div class="direction-l">
            <div class="flag-wrapper">
                <span class="flag">Iteration 9</span>
                <span class="time-wrapper"><span class="time">Febuary 7th</span></span>
            </div>
            <div class="desc">The base was completely disassembled with a new design being implemented, mainly the addition of using L-shaped lego clips that would hold the corners and middle struts in place instead relying solely on interleaved lego blocks. This partially improved the weight issues from the previous iteration but the base was still bending under weight. Two more ultrasonic sensors were added (front left and front right) to increase the object avoidance capabilities.</div>
        </div>
    </li>
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 10</span>
                <span class="time-wrapper"><span class="time">Febuary 15th</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base5.jpg"/> The most radical and pivotal design change took place here, resulting in the wheels which used to be situated on the side of the robot now being directly underneath the robot base. This meant that the weight of the robot was bearing directly on the wheels and no longer on the base resulting in drastic weight bearing capabilities and prevented the lego base from bending. A byproduct of the wheel placement change resulted in increasing the bases ground clearance to 9cm. Another change was the addition of an infrared sensor which was attached to the front of the robot and allowed for the testing of the “follow me” functionality.</div>
        </div>
    </li>
    <li>
        <div class="direction-l">
            <div class="flag-wrapper">
                <span class="flag">Iteration 11</span>
                <span class="time-wrapper"><span class="time">March 1st</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base6.jpg"/>This iteration was less to do with design changes but rather implemented the Raspberry Pi onto the robot base after a testing comparison between the EV3 and Raspberry Pi showed that the Pi controlling the motors would not impact the robots performance drastically. The EV3 is still used for reading sensor data, but the Pi will be processing that data and controlling the robot as a whole.</div>
        </div>
    </li>
    <li>
        <div class="direction-r">
            <div class="flag-wrapper">
                <span class="flag">Iteration 12</span>
                <span class="time-wrapper"><span class="time">March 8th</span></span>
            </div>
            <div class="desc"><img src="/images/lego_base777777.jpg"/> The final iteration of the robot base entailed creating housing under the robot base to hold the Rasbperry Pi and the EV3 in order to make the base top as flat as possible for the arrival of the lift. The lift was intergrated into the robot base and secured, making this iteration the final product.</div>
        </div>
    </li>

    </ul>

    <hr style="height:3px;border:none;color:#333;background-color:#333;">
    <a name="lift"></a>
    <center><h1>Robot Lift</h1></center>
    <h4>Lift Hardware Design</h4>

    <p class="tab"> The lift’s design was developed using the SOLIDWORKS CAD modelling package. SOLIDWORKS includes several useful features which were used in the development of the system. Some of these features include motion studies to detect collision between components through the lift's range of motion, mass property evaluation to estimate the weight of the lift, and clearance verification to determine if there’s enough clearance between components.
    </p>
    <p class="tab"> 
      Utilising these features allowed us to present a design which required minimal modification before being constructed in the mechanical workshop. The final design produced in the workshop was made from MDF and aluminium to ensure strength and stability.
    </p>

    <center>
        <h4>Video of the completed SOLIDWORKS design</h4>
        <video width="480" height="371" controls loop autoplay>
        <source src="/videos/Finished_Side_Profile.mp4" type="video/mp4">
        </video>
        <h4>Completed product</h4>
        <div class="row">
            <div class="column"><img src="/images/lift_retracted.jpg" height="400" width="400"></div>
            <div class="column"><img src="/images/lift_extended.jpg" height="400" width="400"></div>
        </div>
    </center>

    <h4>Lift Drive System</h4> 
    <p class="tab"> The chosen drive system for the lift was a threaded rod design. Utilising a threaded rod design allowed us to provide a mechanism for moving the lift up and down without buckling under the weight of whatever was placed on the platform. It also means less effort is required to move the platform compared to a design that is moving directly against the forces exerted by the lift.</p>
      
    <h4>Lift Electronics</h4>
    <p class="tab"> The lift uses a custom H-bridge circuit connected to the robot’s Raspberry Pi to control the movement of the motor. The circuit is made up of 6 transistors, 4 diodes
          , and 6 resistors. It was designed and simulated within the LTspice simulation program using transistor models imported for the specific components chosen for the project to 
          ensure accurate simulation results.</p>
      
    <img src="/images/H-Bridge_Revised.PNG">

    <hr style="height:3px;border:none;color:#333;background-color:#333;">
    <a name="pi_ev3"></a>
    <center><h1>Raspberry pi and EV3</h1></center>
    <div class="tab">Our design separates robot control on Raspberry pi from sensor reading on EV3 brick. 
        This decision is made based on the fact that EV3 has a more compatible interface to hardware components like sensors, but meanwhile constrained by limited computation power. On the other hand, Raspberry pi not only is capable of concurrent multitasking but also has an independent power supply for the motors resulting in more powerful motor control. Therefore, by isolating the tasks 
        on two devices, our robot achieves better weight capacity, simpler design and smoother robot control. </div>
    <h4>Raspberry pi</h4>
    <div class="tab">
        Raspberry pi is the centre of control for the robot. It runs a TCP server connecting with the mobile app and EV3. Based on the sensor data from EV3, it interprets the command issued by the mobile app and changes the state of the robot accordingly. 
    </div>

    <center>
        <h4>Raspberry pi system diagram</h4>
        <div>Click on the image's rectanglar region to learn more</div>
        <img src="/images/server_diagram.png" alt="server" usemap="#systemmap">
    </center>
    <map name="systemmap">
        <area shape="rect" coords="0,0,400,360" href="#setup">
        <area shape="rect" coords="0,365,400,550" href="#idle">
        <area shape="rect" coords="0,550,635,1070" href="#control">
        <area shape="rect" coords="0,1075,400,1355" href="#finish">
    </map>
    <ul>
        <li><a name="setup"></a><b>Set up stage</b><br>
            When the user turns on the robot, the program on Raspberry pi will be invoked automatically on system boot and listen on TCP connection from EV3 brick.
            The robot's set up time is essential the time for Raspberry pi's operating system to finish booting plus the time for EV3 connecting to Raspberry pi's server.
            Once it's done, the server will decode the message received, convert it to sensor data and temporarily store in a local variable.
            The processing of data received from EV3 is done in the background without disrupting the normal control flow of the robot to maximize its performance. 
            Rather than initiatively request for sensor data from EV3, it will be updated on new data’s arrival. This guarantees the control of the motors is
            always smooth and continuous and reduces the communication overhead for sensor data to be propagated from EV3 to Raspberry pi.
        </li>
        <li><a name="idle"></a><b>Idle mode</b><br>
            Without blocking on EV3's data, the server will proceed to listen on the connection from the mobile app. If there's no connection, it will stop at this stage until the connection is established. Indeed, it is usually not the case that the user uses the robot immediately after turning it on, but as long as the robot is switched on without any exception intervene the normal flow of the program, it can react to incoming connection right away. Once it is connected with mobile app, Raspberry pi is ready to carry out commands from app. Depends on the type of commands, the robot will enter into three different modes of control.
        </li>
        <li><a name="control"></a><b>Manual control mode</b><br>
            If the command is an explicit direction, i.e. one of “forward”, ”backward”, ”right”, ”left”, “forward-right”, “forward-left”, “backward-right” and “backward-left”, the wheel’s 
            motor controller will rotate motors at a given speed in combination to move the robot in that direction. It will keep moving until either the app issues a “stop” command or 
            there are objects in front of the robot.
        </li>
        <li><b>Follow mode</b><br>
            If the command is “follow”, the controller will examine the infrared sensor data to determine the user’s orientation relative to the robot and rotate the robot to face the user. 
            It then moves towards that direction until its distance with respect to the infrared beacon is less than 30 cm. While approaching the user, it keeps checking whether there are 
            obstacles blocking its path. If there exists any object within 50 cm in front of the robot, the controller will trigger the object avoidance mechanism to move around that object. 
            To be specific, the robot will rotate either right or left depending on which side doesn’t detect objects based on the data from auxiliary ultrasonic sensors on both sides. 
            <br>If the object blocks the whole range of area the robot’s sensors cover, the robot will stop and send a message to the app. If the user moves too fast for the robot to follow resulting 
            in the infrared sensor losing the beacon’s signal, the robot will also stop and raise an exception to the app. The controller on Raspberry pi will stop automatic following if the 
            user issues a “stop following” command.
        </li>
        <li><b>Lift control mode</b><br>
            For the sake of safety, the robot is only allowed to move when the lift is at base height, i.e. fully folded. When the user opens the lift control panel on the app, Raspberry pi 
            will receive a notification on it and unlock the control of the lift motor. Depending on the command, it moves the lift up or down. Once the app relays that the user exits from 
            lift control mode, it will check whether the lift is in base height. If not, it will automatically fold the lift before switching to another mode.
        </li>
        <li><a name="finish"></a><b>Finish up stage</b><br>
            When the user closes the app, Raspberry pi will automatically disconnects with app. Depends on whether the user switches off the robot, it will either revert to idle 
            mode and wait for next connection or shut down the server.
        </li>
    </ul>
    <h4>EV3 brick</h4>
    <div class="tab">EV3 brick is connected to four sensors: an infrared sensor and three ultrasonic sensors. </div> 
    <div class="two_columns">
        <div class="column">
            <ul>
                <li>
                    The program on EV3 continuously reads the distance-from-object data from the ultrasonic sensors as well as angle and distance to the portable beacon from the infrared sensor.
                </li>
                <li>The TCP client running on EV3 automatically connected to the server on Raspberry pi when EV3 is turned on. Then, it organises sensor data into a package and sends to the
                     server continuously.
                    </li>
            </ul>
          </div>
      <div class="column">
      </div>
    <center>
        <h4>EV3 system diagram</h4>
        <div>click to zoom in</div>
        <a href="/images/client_diagram.png"><img src="/images/client_diagram.png" alt="client" usemap="#clientmap" width="15%" height="10%"></a>
    </center>
    </div>
    <ul>
        <li>
            The program on EV3 continuously reads the distance-from-object data from the ultrasonic sensors as well as angle and distance to the portable beacon from the infrared sensor.
        </li>
        <li>The TCP client running on EV3 automatically connected to the server on Raspberry pi when EV3 is turned on. Then, it organises sensor data into a package and sends to the
             server continuously.
            </li>
    </ul>

    <hr style="height:3px;border:none;color:#333;background-color:#333;">
    <a name="app"></a>
    <center><h1>Mobile Application</h1></center>

    <p class="tab"> The primary purpose of the app is to be the main source of communication between the user and their N.E.A.T. The app uses TCP/IP protocol to send commands to 
        the robot and receive feedback which is used to inform the user of any issues the robot has encountered during use.
    </p>
    <h4>Mobile App Design</h4>
    <p class="tab">
        The app was designed with usability and accessibility for all as a priority. Therefore it complies with W3C accessibility guidelines, resulting in an app which is fully compatible 
        with screen readers and other accessibility features offered by modern smartphones. In addition to our focus on making the app accessible to all users, we have also 
        implemented a simple and easy to use interface to ensure even those who are less “tech-savvy” can take full advantage of all the features their N.E.A.T. has to offer. Some of the             
        key features of our interface include at least two types of feedback from all buttons: haptic (vibrate on touch), audio and visual as appropriate for that action; as well as the option 
        to use voice commands to control your N.E.A.T. By streamlining the design to focus on the goals of the user, we have developed an app that we hope everyone loves to use.
    </p>
    <div style="float: left;">
        <h4>Main Menu</h4>
        <p class="tab">
            The initial landing screen provides a pop-up which allows a user to connect their mobile app to their N.E.A.T. Once they have done so, they are ready to control their robot. 
            The main menu consists of four buttons, as seen below. Each button offers both haptic and audio feedback. A different audio tone plays for each to improve the ease with which a 
            user can identify which button they have pressed. The buttons are large and differently coloured, to help to identify them. The large text makes it easy to read, and those buttons 
            which lead to a new screen on the app have an arrow icon to indicate this to a user.
        </p>
        <center><div class="row">
            <div class="column"><img src="/images/connect_app.jpg" width="55%"/></div>
            <div class="column"><img src="/images/menu_app.jpg" width="55%"/></div>
        </div></center>
    </div>
    <div style="float: left;">
        <h4>Voice Controls</h4>
        <p class="tab" style="width: 60%; float: left; margin-right: 20px;">
            In the interest of ensuring our app is accessible to all, the “use voice commands” button on the main menu allows a user to control their N.E.A.T. using voice commands. Once they 
            click this button, they are brought to a new screen which is ready to receive voice commands. There is a set list of commands to control all the functionality of the robot. When the 
            app recognises a command, it will play a success tone and show the command on the screen. If the app doesn’t recognise a command, it will respond with an error tone and prompt the user to 
            try again. To stop using voice control and return to the use of the app interface, the user must simply exit this screen.
        </p>
        <img src="/images/voice_app.jpg" style="width: 25%;"/>
    </div>
    <div style="float: left;"></div>
        <h4>Movement Options</h4>
        <div style="margin-bottom: 40px;">
            <p class="tab" style="width: 60%; float: left; margin-right: 20px; margin-bottom: 10px;">
                There are two different modes of movement offered via the main menu: manual control of your N.E.A.T. and use of the “follow me” feature. Selecting “manual control” brings the user to a 
                new screen with a control panel which allows a user to move the robot in eight different directions and rotate, as shown to the right. Using a simple control interface where the robot moves 
                while a direction button is being held, and stops once it is released, the user can easily control the robot. Each of these buttons offers both haptic and visual feedback, with the colour of the button changing while pressed.
            </p>
            <img src="/images/manual_app.jpg" style="width: 25%;"/>
        </div>
        <p class="tab">
            The alternative movement option is the “follow me” mode, one of the main features of N.E.A.T. Once the user has the portable infrared beacon turned on, they can start the “follow me” 
            feature with one simple click of a button. Once clicked, the “follow me” button will change to show “stop following” with a red background, to make it easy for a user to stop the robot 
            once they have reached their destination. The robot will then follow the user to their destination until the user taps the “stop following” button, which reverts to showing “follow me” 
            once the robot has stopped.
        </p>
        <center><div class="row">
            <div class="column"><img src="/images/menu_app.jpg" width="55%"/></div>
            <div class="column"><img src="/images/stop_follow_app.jpg" width="55%"/></div>
        </div></center>
    <div style="float: left;">
        <h4>Lift Controls</h4>
        <p class="tab" style="width: 60%; float: left; margin-right: 20px;">
            The final button on the main menu, “lift controls” allows the user to change the height of the lift. Once clicked, it leads the user to a new screen which offers both manual control of 
            the lift height and various preset heights. The manual controls work as they did for manual movement, with the lift moving while the button is pressed and stop once it is released. 
            Once again, these buttons offer haptic and visual feedback. The preset heights include counter height, table height and base height. Counter height and table height are set to the averages 
            heights for each, 91cm and 76cm respectively (as found <a href="http://www.parotas.com/en/standard-table-chair-heights-guide/">here</a>), while base height lowers the lift to be fully folded against the 
            robot base. These buttons respond with haptic feedback and individual audio tones to distinguish them from one another.
        </p>
        <img src="/images/lift_app.jpg" style="width: 25%; margin-top: 70px;"/>
    </div>
    <div style="float: left;">
        <h4>Robot Feedback</h4>
        <p class="tab">
            The final key feature of the app is that it provides feedback from the robot to the user. The main two issues we deal with are that of the infrared beacon signal being lost while the 
            robot is following a user, or the robot finding itself unable to avoid an obstacle in its path. In both cases, a pop-up appears on the app to inform the user and offer guidance on how 
            to resolve it. When the beacon is lost, the pop-up suggests the user move within range of the infrared sensor, or offers the option to briefly switch to manual control to bring the robot closer to the user. In the case of an unavoidable obstacle, the pop-up encourages the user to either manually remove the obstacle or switch to manual control to guide the robot around the object.
        </p>
        <center>
        <div class="row">
            <div class="column"><img src="/images/beacon_app.jpg" width="55%"/></div>
            <div class="column"><img src="/images/obstacle_app.jpg" width="55%"/></div>
        </div></center>
    </div>
    
</body>
{% include long_page.html %}
</html>
